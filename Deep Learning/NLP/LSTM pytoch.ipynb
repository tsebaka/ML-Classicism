{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "!git clone https://github.com/tsebaka/ML-from-scratch\n",
        "import sys\n",
        "sys.path.insert(0,'/content/ML-from-scratch/Parse-lib')\n",
        "import numpy as np\n",
        "import nltk\n",
        "import bs4 as bs\n",
        "import re\n",
        "import urllib.request\n",
        "import warnings\n",
        "import pp\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ro7-OzI-tEf4",
        "outputId": "7dcc3bbc-93f4-4a17-ccae-05fdbe8f141b"
      },
      "execution_count": 394,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ML-from-scratch' already exists and is not an empty directory.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 394
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sites =[#'https://breakingbad.fandom.com/wiki/Walter_White',\n",
        "         'https://en.wikipedia.org/wiki/Breaking_Bad',\n",
        "        #  'https://breakingbad.fandom.com/wiki/Jesse_Pinkman',\n",
        "        #  'https://breakingbad.fandom.com/wiki/Gustavo_Fring', \n",
        "        # 'https://breakingbad.fandom.com/wiki/Jimmy_McGill',\n",
        "        # 'https://breakingbad.fandom.com/wiki/Mike_Ehrmantraut',\n",
        "        # 'https://breakingbad.fandom.com/wiki/Skyler_White',\n",
        "        # 'https://breakingbad.fandom.com/wiki/Hank_Schrader'\n",
        "         ]\n",
        "\n",
        "parser = pp.parse_and_prepare(EOS=True)\n",
        "vocab, text, all_word = parser.parse(sites)"
      ],
      "metadata": {
        "id": "Ism4HvYwkR1z"
      },
      "execution_count": 395,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "\n",
        "model = gensim.models.Word2Vec(all_word,\n",
        "                              min_count=1, \n",
        "                              workers=4,\n",
        "                              size=50,\n",
        "                              window=3,\n",
        "                              iter=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QIU-xaqEZir",
        "outputId": "20fc6d1e-1335-4300-d83c-401d7abeb8e9"
      },
      "execution_count": 396,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.base_any2vec:consider setting layer size to a multiple of 4 for greater performance\n",
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv['jesse']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOLhVjUiysVc",
        "outputId": "d46d5151-1f71-4b1a-d935-714febaee699"
      },
      "execution_count": 397,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.80519515, -0.01865546,  0.34359923,  0.4018504 , -0.01313855,\n",
              "       -0.18804446,  0.09777899,  0.03102148, -0.04293525,  0.02108033,\n",
              "       -0.08953659, -0.12652211, -0.31095377,  0.21592027,  0.10771915,\n",
              "        0.42813733, -0.05000349,  0.22815248,  0.29980892, -0.27068007,\n",
              "        0.35995916,  0.18484813, -0.03201825, -0.12530313,  0.47517127,\n",
              "       -0.12698232, -0.07086036,  0.17725305,  0.26749054, -0.5129367 ,\n",
              "        0.23906566,  0.3105898 , -0.25617987,  0.23161663, -0.08303642,\n",
              "       -0.3416912 , -0.06867372,  0.31836742, -0.14585724,  0.06097819,\n",
              "        0.13171798,  0.17051606,  0.18732682,  0.2501148 , -0.38491118,\n",
              "        0.2656173 , -0.02340393, -0.12001038,  0.10801007, -0.6975137 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 397
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "ZaEnvVaPkg88"
      },
      "execution_count": 398,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = np.unique(text)"
      ],
      "metadata": {
        "id": "rubnnOhEFyz4"
      },
      "execution_count": 399,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diction = defaultdict(lambda: len(vocab))\n",
        "idx_to_word = defaultdict(lambda: len(vocab))\n",
        "\n",
        "for idx, word in enumerate(vocab):\n",
        "    diction[word] = idx\n",
        "    idx_to_word[idx] = word"
      ],
      "metadata": {
        "id": "t5d188jQwaAC"
      },
      "execution_count": 400,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_to_word[2193]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "SkQ0k84f-ZkV",
        "outputId": "3b52e0f8-bbe5-4709-e13f-0525b6ac90ac"
      },
      "execution_count": 401,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'walt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 401
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diction['walt']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1MRI7fMxZCo",
        "outputId": "ae6e7be9-d8cc-49c5-a1ba-987767cfba8f"
      },
      "execution_count": 402,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2193"
            ]
          },
          "metadata": {},
          "execution_count": 402
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_word[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPoeRqYuwBwO",
        "outputId": "c0da8b1d-2f8d-40fe-b858-91ba86cd37a7"
      },
      "execution_count": 403,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['breaking',\n",
              " 'bad',\n",
              " 'is',\n",
              " 'an',\n",
              " 'american',\n",
              " 'crime',\n",
              " 'drama',\n",
              " 'television',\n",
              " 'series',\n",
              " 'created',\n",
              " 'and',\n",
              " 'produced',\n",
              " 'by',\n",
              " 'vince',\n",
              " 'gilligan',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 403
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = []"
      ],
      "metadata": {
        "id": "WXOAu1ZpzEfo"
      },
      "execution_count": 404,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_word[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czrL2qhe1txk",
        "outputId": "857a042e-02e7-4f68-9851-3c59c2c47528"
      },
      "execution_count": 405,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['breaking',\n",
              " 'bad',\n",
              " 'is',\n",
              " 'an',\n",
              " 'american',\n",
              " 'crime',\n",
              " 'drama',\n",
              " 'television',\n",
              " 'series',\n",
              " 'created',\n",
              " 'and',\n",
              " 'produced',\n",
              " 'by',\n",
              " 'vince',\n",
              " 'gilligan',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 405
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(all_word)):\n",
        "    target = []\n",
        "    for j in range(len(all_word[i]) - 1):\n",
        "        target.append(all_word[i][j + 1])\n",
        "\n",
        "    all_word[i].pop()\n",
        "    data.append((all_word[i], target))"
      ],
      "metadata": {
        "id": "SSm2yMX3y1XD"
      },
      "execution_count": 406,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fyk2UjtEH9fn",
        "outputId": "cb15a237-e846-4a09-ca42-1e2aa5157925"
      },
      "execution_count": 407,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "425"
            ]
          },
          "metadata": {},
          "execution_count": 407
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_set = data[350:425]\n",
        "training_set = data[0:350]"
      ],
      "metadata": {
        "id": "hTU7abkm1d8L"
      },
      "execution_count": 408,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        # Recurrent layer\n",
        "        # YOUR CODE HERE!\n",
        "        self.lstm = nn.LSTM(input_size=len(vocab) + 1,\n",
        "                         hidden_size=50,\n",
        "                         num_layers=1,\n",
        "                         bidirectional=False)\n",
        "        \n",
        "        # Output layer\n",
        "        self.l_out = nn.Linear(in_features=50,\n",
        "                            out_features=len(vocab) + 1,\n",
        "                            bias=False)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # RNN returns output and last hidden state\n",
        "        x, (h, c) = self.lstm(x)\n",
        "        \n",
        "        # Flatten output for feed-forward layer\n",
        "        x = x.view(-1, self.lstm.hidden_size)\n",
        "        \n",
        "        # Output layer\n",
        "        x = self.l_out(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sy14sE4Jvkgc",
        "outputId": "b32dfbdf-9a50-4dc6-dc04-dce9b2497286"
      },
      "execution_count": 409,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (lstm): LSTM(2267, 50)\n",
            "  (l_out): Linear(in_features=50, out_features=2267, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(idx, vocab_size):\n",
        "    \"\"\"\n",
        "    One-hot encodes a single word given its index and the size of the vocabulary.\n",
        "    \n",
        "    Args:\n",
        "     `idx`: the index of the given word\n",
        "     `vocab_size`: the size of the vocabulary\n",
        "    \n",
        "    Returns a 1-D numpy array of length `vocab_size`.\n",
        "    \"\"\"\n",
        "    # Initialize the encoded array\n",
        "    one_hot = np.zeros(vocab_size)\n",
        "    \n",
        "    # Set the appropriate element to one\n",
        "    one_hot[idx] = 1.0\n",
        "\n",
        "    return one_hot\n",
        "def one_hot_encode_sequence(sequence, vocab_size):\n",
        "    \"\"\"\n",
        "    One-hot encodes a sequence of words given a fixed vocabulary size.\n",
        "    \n",
        "    Args:\n",
        "     `sentence`: a list of words to encode\n",
        "     `vocab_size`: the size of the vocabulary\n",
        "     \n",
        "    Returns a 3-D numpy array of shape (num words, vocab size, 1).\n",
        "    \"\"\"\n",
        "    # Encode each word in the sentence\n",
        "    encoding = np.array([one_hot_encode(diction[word], vocab_size) for word in sequence])\n",
        "\n",
        "    # Reshape encoding s.t. it has shape (num words, vocab size, 1)\n",
        "    encoding = encoding.reshape(encoding.shape[0], encoding.shape[1], 1)\n",
        "    \n",
        "    return encoding"
      ],
      "metadata": {
        "id": "x1WAY76VCVRV"
      },
      "execution_count": 410,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper-parameters\n",
        "num_epochs = 50\n",
        "\n",
        "# Initialize a new network\n",
        "net = Net()\n",
        "\n",
        "# Define a loss function and optimizer for this problem\n",
        "# YOUR CODE HERE!\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=3e-4)\n",
        "\n",
        "# Track loss\n",
        "training_loss, validation_loss = [], []\n",
        "\n",
        "# For each epoch\n",
        "for i in range(num_epochs):\n",
        "    \n",
        "    # Track loss\n",
        "    epoch_training_loss = 0\n",
        "    epoch_validation_loss = 0\n",
        "    \n",
        "    net.eval()\n",
        "    cnt = 0\n",
        "    # For each sentence in validation set\n",
        "    for inputs, targets in validation_set:\n",
        "        # One-hot encode input and target sequence\n",
        "        # embeds = []\n",
        "        # for word in inputs:\n",
        "        #     embeds.append(model.wv[word].transpose())\n",
        "        # embeds = np.array(embeds)\n",
        "        # embeds = embeds.transpose()\n",
        "        # embeds = np.array([embeds])\n",
        "        embeds = []\n",
        "        for k in range(len(inputs)):\n",
        "            one_hot = [0] * (len(vocab) + 1)\n",
        "            # print(inputs[k])\n",
        "            one_hot[diction[inputs[k]]] = 1\n",
        "            embeds.append(one_hot)\n",
        "        embeds = np.array(embeds)\n",
        "        embeds = embeds.transpose()\n",
        "        embeds = np.array([embeds])\n",
        "        targets_idx = [diction[word] for word in targets]\n",
        "        \n",
        "        embeds = torch.Tensor(embeds)\n",
        "        # print(embeds.shape)\n",
        "        embeds = embeds.permute(0, 2, 1)\n",
        "        targets_idx = torch.LongTensor(targets_idx)\n",
        "\n",
        "        outputs = net.forward(embeds)\n",
        "\n",
        "        loss = criterion(outputs, targets_idx)\n",
        "\n",
        "        epoch_validation_loss += loss.detach().numpy()\n",
        "    \n",
        "    net.train()\n",
        "    \n",
        "    # For each sentence in training set\n",
        "    cnt = 0\n",
        "    for inputs, targets in training_set:\n",
        "        # cnt += 1\n",
        "        # if cnt < 276:\n",
        "        # One-hot encode input and target sequence\n",
        "        embeds = []\n",
        "        for k in range(len(inputs)):\n",
        "            one_hot = [0] * (len(vocab) + 1)\n",
        "            # print(inputs[k])\n",
        "            one_hot[diction[inputs[k]]] = 1\n",
        "            embeds.append(one_hot)\n",
        "        embeds = np.array(embeds)\n",
        "        embeds = embeds.transpose()\n",
        "        embeds = np.array([embeds])\n",
        "        targets_idx = [diction[word] for word in targets]\n",
        "        \n",
        "        # Convert target to tensor\n",
        "        embeds = torch.Tensor(embeds)\n",
        "        embeds = embeds.permute(0, 2, 1)\n",
        "        targets_idx = torch.LongTensor(targets_idx)\n",
        "        # Forward pass\n",
        "        \n",
        "        # YOUR CODE HERE!\n",
        "        outputs = net.forward(embeds)\n",
        "        \n",
        "        # Compute loss\n",
        "        # YOUR CODE HERE!\n",
        "\n",
        "        loss = criterion(outputs, targets_idx)\n",
        "        \n",
        "        # Backward pass\n",
        "        # YOUR CODE HERE!\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Update loss\n",
        "        epoch_training_loss += loss.detach().numpy()\n",
        "        # else:\n",
        "        #     break\n",
        "        \n",
        "    # Save loss for plot\n",
        "    training_loss.append(epoch_training_loss/len(training_set))\n",
        "    validation_loss.append(epoch_validation_loss/len(validation_set))\n",
        "\n",
        "    # Print loss every 5 epochs\n",
        "    if i % 5 == 0:\n",
        "        print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbQWr6urvZIz",
        "outputId": "8355aa91-0191-4f68-83b6-b88fef0281ae"
      },
      "execution_count": 411,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, training loss: 7.515769438062395, validation loss: 7.7183026059468585\n",
            "Epoch 5, training loss: 4.614141505445753, validation loss: 5.108729257583618\n",
            "Epoch 10, training loss: 4.525316413130079, validation loss: 5.206770779291789\n",
            "Epoch 15, training loss: 4.437944059712546, validation loss: 5.296433534622192\n",
            "Epoch 20, training loss: 4.346894176006317, validation loss: 5.381518402099609\n",
            "Epoch 25, training loss: 4.255597752843585, validation loss: 5.491906579335531\n",
            "Epoch 30, training loss: 4.150505072729928, validation loss: 5.624347235361735\n",
            "Epoch 35, training loss: 4.038934882708959, validation loss: 5.764317282040914\n",
            "Epoch 40, training loss: 3.932171310697283, validation loss: 5.891244384447734\n",
            "Epoch 45, training loss: 3.8311471029690334, validation loss: 5.991313852469126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diction['is']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uMlqlShEDZT",
        "outputId": "62e21f33-1d40-4bd8-d66e-0e4ea34530e5"
      },
      "execution_count": 412,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2266"
            ]
          },
          "metadata": {},
          "execution_count": 412
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx_to_word[2266]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hLHLWFKD4ZS",
        "outputId": "ab184529-d722-4858-8393-1df474bbb2c5"
      },
      "execution_count": 413,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2266"
            ]
          },
          "metadata": {},
          "execution_count": 413
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get first sentence in test set\n",
        "inputs, targets = validation_set[1]\n",
        "targets = ['cranston']\n",
        "inputs = ['bryan']\n",
        "# One-hot encode input and target sequence\n",
        "embeds = []\n",
        "for word in inputs:\n",
        "    embeds.append(model.wv[word])\n",
        "targets_idx = [diction[word] for word in targets]\n",
        "\n",
        "# Convert input to tensor\n",
        "embeds = []\n",
        "for k in range(len(inputs)):\n",
        "    one_hot = [0] * (len(vocab) + 1)\n",
        "    # print(inputs[k])\n",
        "    one_hot[diction[inputs[k]]] = 1\n",
        "    embeds.append(one_hot)\n",
        "embeds = np.array(embeds)\n",
        "embeds = embeds.transpose()\n",
        "embeds = np.array([embeds])\n",
        "targets_idx = [diction[word] for word in targets]\n",
        "\n",
        "# Convert target to tensor\n",
        "embeds = torch.Tensor(embeds)\n",
        "embeds = embeds.permute(0, 2, 1)\n",
        "targets_idx = torch.LongTensor(targets_idx)\n",
        "\n",
        "# Forward pass\n",
        "# YOUR CODE HERE!\n",
        "outputs = net.forward(embeds).data.numpy()\n",
        "\n",
        "print('\\nInput sequence:')\n",
        "print(inputs)\n",
        "\n",
        "print('\\nTarget sequence:')\n",
        "print(targets)\n",
        "\n",
        "print('\\nPredicted sequence:')\n",
        "# print([idx_to_word[np.argmax(outputs)] for output in outputs])\n",
        "for i in range(len(outputs)):\n",
        "    print(idx_to_word[np.argmax(outputs[i][0:2260])])\n",
        "# Plot training and validation loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVRg1wGx8liB",
        "outputId": "7275e855-2927-4b9f-d3ea-f607018c3e91"
      },
      "execution_count": 423,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input sequence:\n",
            "['bryan']\n",
            "\n",
            "Target sequence:\n",
            "['cranston']\n",
            "\n",
            "Predicted sequence:\n",
            "cranston\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}