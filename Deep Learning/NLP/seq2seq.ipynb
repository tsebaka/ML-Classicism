{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EIlaym0ENxMH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import re\n",
        "\n",
        "import random\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "pyNecl7SPlAn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source = torch.tensor([[1, 2, 3, 1], [1, 1, 2, 3]], dtype=torch.float32)\n",
        "target = torch.tensor([1, 1], dtype=torch.long)\n",
        "criterion(source, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAfTK_NZPw8U",
        "outputId": "8a4e5c97-bd94-46de-839a-ce747159c9fd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.9938)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source.shape, target.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0ilFLJjP-4S",
        "outputId": "12b0848e-57b5-4170-88db-da54fdddfc3f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 4]), torch.Size([2]))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Dictionary():\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2idx = {'<sos>': 0, '<eos>': 1, '<unk>': 2}\n",
        "        self.word2count = {'<sos>': 1, '<eos>': 1, '<unk>': 1}\n",
        "        self.idx2word = {0: '<sos>', 1: '<eos>', 2: '<unk>'}\n",
        "        self.num_words = 3\n",
        "        \n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split():\n",
        "            self.addWord(word)\n",
        "        \n",
        "    def addWord(self, word):\n",
        "        if not word in self.word2idx:\n",
        "            self.word2idx[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.idx2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        \n",
        "        return self.num_words"
      ],
      "metadata": {
        "id": "PJGj0gMKY8nz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Corpus():\n",
        "    def __init__(self, source_name, target_name):\n",
        "        self.source_lang = Dictionary(source_name)\n",
        "        self.target_lang = Dictionary(target_name)\n",
        "    \n",
        "    def get_data(self, path):\n",
        "        pairs = []\n",
        "\n",
        "        with open(path, 'r') as lines:\n",
        "            for line in lines:\n",
        "                pair = line.replace('\\n', '').split(',')\n",
        "                pairs.append(pair)\n",
        "                self.source_lang.addSentence(pair[0])\n",
        "                self.target_lang.addSentence(pair[1])\n",
        "\n",
        "        for num, pair in enumerate(pairs):\n",
        "            for word in pair[0].split():\n",
        "                if self.source_lang.word2count[word] <= 1:\n",
        "                    self.source_lang.num_words -= 1\n",
        "                    pairs[num][0] = re.sub(fr\"\\b{word}\\b\", \"<unk>\", pairs[num][0])\n",
        "                \n",
        "            for word in pair[1].split():\n",
        "                if self.target_lang.word2count[word] <= 1:\n",
        "                    self.target_lang.num_words -= 1\n",
        "                    pairs[num][1] = re.sub(fr\"\\b{word}\\b\", \"<unk>\", pairs[num][1])\n",
        "\n",
        "        return pairs, self.source_lang, self.target_lang\n",
        "    \n",
        "corpus = Corpus('en', 'de')\n",
        "pairs, source_lang, target_lang = corpus.get_data('/content/text (1).txt')\n",
        "random.choice(pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Xzi3lRPYxU3",
        "outputId": "185bc65a-26d0-40b6-8d41-6f32a3b807c7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<sos> two young kids are playing some kind of a game',\n",
              " '<sos> zwei kleine kinder spielen irgendein spiel <eos>']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LEN_SOURCE_VOCAB = source_lang.num_words\n",
        "LEN_TARGET_VOCAB = target_lang.num_words\n",
        "EMBEDDING_SIZE = 10\n",
        "HIDDEN_SIZE = 10\n",
        "N_LAYERS = 1\n",
        "DROPOUT = 0.5\n",
        "LEN_SOURCE_VOCAB, LEN_TARGET_VOCAB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0Y_texic62q",
        "outputId": "40de136e-691a-469e-9db0-af0bca75f29f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5866, 7767)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pair2tensor(pair):\n",
        "    source = torch.tensor([source_lang.word2idx[word] for word in pair[0].split()], dtype=torch.long)\n",
        "    target = torch.tensor([target_lang.word2idx[word] for word in pair[1].split()], dtype=torch.long)\n",
        "\n",
        "    return source, target"
      ],
      "metadata": {
        "id": "Kc05ucrMd_cZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs[32]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DGfnQtUfSPR",
        "outputId": "1ea97926-e6b5-48a7-d02a-513223ff366d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<sos> a old man having a beer alone <eos>',\n",
              " '<sos> ein alter mann der allein ein bier trinkt <eos>']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, n_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size,\n",
        "                                      embedding_size)\n",
        "        \n",
        "        self.rnn = nn.LSTM(embedding_size,\n",
        "                           hidden_size,\n",
        "                           n_layers,\n",
        "                           dropout=dropout)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, source):\n",
        "        embed = self.dropout(self.embedding(source))\n",
        "\n",
        "        output, (hidden, cell) = self.rnn(embed)\n",
        "\n",
        "        return hidden, cell\n",
        "\n",
        "encoder = Encoder(LEN_SOURCE_VOCAB,\n",
        "                  EMBEDDING_SIZE,\n",
        "                  HIDDEN_SIZE,\n",
        "                  N_LAYERS,\n",
        "                  DROPOUT).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDNWqrmGfYMy",
        "outputId": "465f92cb-2ece-4ad5-cff5-9c3907bdf16b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_size, embedding_size, hidden_size, n_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size,\n",
        "                                      embedding_size)\n",
        "        \n",
        "        self.rnn = nn.LSTM(embedding_size,\n",
        "                           hidden_size,\n",
        "                           n_layers,\n",
        "                           dropout=dropout)\n",
        "        \n",
        "        self.fc1 = nn.Linear(hidden_size,\n",
        "                             output_size)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        embed = self.dropout(self.embedding(input))\n",
        "\n",
        "        output, (hidden, cell) = self.rnn(embed, (hidden, cell))\n",
        "\n",
        "        prediction = self.fc1(output)\n",
        "\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "decoder = Decoder(LEN_TARGET_VOCAB,\n",
        "                  EMBEDDING_SIZE,\n",
        "                  HIDDEN_SIZE,\n",
        "                  N_LAYERS,\n",
        "                  DROPOUT).to(device)"
      ],
      "metadata": {
        "id": "fkbPMZJvpTd-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pair = ['<sos> a bride and groom make small talk with a guest as the groom shakes the guest s hand <eos>',\n",
        " '<sos> braut und brautigam beim <unk> mit einem gast und der brautigam schuttelt dem gast die hand <eos>']"
      ],
      "metadata": {
        "id": "YeCz9XKZ0gAB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YI_luNGc5nj5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
        "        \n",
        "        hidden, cell = self.encoder(source)\n",
        "        \n",
        "        word = target[0].view(-1)\n",
        "\n",
        "        outputs = torch.zeros(len(target) - 1, decoder.output_size).to(device)\n",
        "\n",
        "        for idx in range(0, len(target) - 1):\n",
        "            output, hidden, cell = decoder.forward(word.view(-1), hidden, cell)\n",
        "\n",
        "            outputs[idx] = output\n",
        "\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            prediction = output.argmax(1)\n",
        "\n",
        "            word = target[idx + 1] if teacher_forcing_ratio else prediction\n",
        "\n",
        "        return outputs\n",
        "\n",
        "seq2seq = Seq2seq(encoder, decoder).to(device)"
      ],
      "metadata": {
        "id": "CvFsD2QfxJEf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "seq2seq.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxC6K3CLU95b",
        "outputId": "33c0b058-b969-4923-b4be-83be77c7c510"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(5866, 10)\n",
              "    (rnn): LSTM(10, 10, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(7767, 10)\n",
              "    (rnn): LSTM(10, 10, dropout=0.5)\n",
              "    (fc1): Linear(in_features=10, out_features=7767, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(seq2seq):,} trainable parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKR-xS1VQqTQ",
        "outputId": "a30814ed-9865-4211-a144-d71f36deb8e7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 223,527 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_enc = optim.Adam(encoder.parameters(), lr=0.01)\n",
        "optimizer_dec = optim.Adam(decoder.parameters(), lr=0.01)\n",
        "teacher_forcing_ratio = 0.5"
      ],
      "metadata": {
        "id": "NM7eKRa_Qj9p"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_epoch = 10\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for pair in pairs:\n",
        "        if (len(pair[0].split()) and len(pair[1].split()) <= 17):\n",
        "            source, target = pair2tensor(pair)\n",
        "            source, target = source.to(device), target.to(device)\n",
        "\n",
        "            trg = target\n",
        "\n",
        "            optimizer_enc.zero_grad()\n",
        "            optimizer_dec.zero_grad()\n",
        "\n",
        "            hidden, cell = encoder(source)\n",
        "            \n",
        "            word = trg[0].view(-1)\n",
        "\n",
        "            outputs = torch.zeros(len(trg) - 1, decoder.output_size).to(device)\n",
        "\n",
        "            for idx in range(0, len(trg) - 1):\n",
        "                output, hidden, cell = decoder.forward(word.view(-1), hidden, cell)\n",
        "\n",
        "                outputs[idx] = output\n",
        "\n",
        "                teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "                prediction = output.argmax(1)\n",
        "\n",
        "                word = trg[idx + 1] if teacher_forcing_ratio else prediction\n",
        "\n",
        "            output = outputs\n",
        "\n",
        "            loss = criterion(output, target[1:])    \n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(encoder.parameters(), 1)\n",
        "            torch.nn.utils.clip_grad_norm_(decoder.parameters(), 1)\n",
        "            \n",
        "            optimizer_enc.step()\n",
        "            optimizer_dec.step()\n",
        "            \n",
        "            epoch_loss += loss.item()   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "xOZIOOj47CD7",
        "outputId": "b23aeeca-1042-42bb-9176-5bb84b5d4a0a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-5c61832948c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_epoch = 10\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "    seq2seq.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for pair in pairs:\n",
        "        if (len(pair[0].split()) and len(pair[1].split()) <= 17):\n",
        "            source, target = pair2tensor(pair)\n",
        "            source, target = source.to(device), target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = seq2seq.forward(source, target).to(device)\n",
        "\n",
        "\n",
        "            loss = criterion(output, target[1:])\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(seq2seq.parameters(), 1)\n",
        "            \n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    print(epoch_loss)"
      ],
      "metadata": {
        "id": "ujpo-_S0QhYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pair"
      ],
      "metadata": {
        "id": "KRU7fQk8gLST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq.eval()"
      ],
      "metadata": {
        "id": "9tk2Bbqwf5sX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos = 20\n",
        "source, target = pair2tensor(pairs[pos])\n",
        "print(pairs[pos][0])\n",
        "print(pairs[pos][1])\n",
        "source, target"
      ],
      "metadata": {
        "id": "-AHe7WNWWi9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = seq2seq.forward(source.to(device), target[:-1].to(device))"
      ],
      "metadata": {
        "id": "fJfT1p4JSaem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.argmax(1)"
      ],
      "metadata": {
        "id": "tA6tAIYFUE6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IvU9OsdoUGDX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}