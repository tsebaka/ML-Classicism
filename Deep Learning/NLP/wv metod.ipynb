{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tsebaka/ML-from-scratch\n",
        "import sys\n",
        "sys.path.insert(0,'/content/ML-from-scratch/Parse-lib')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPFQ-ufR7P0K",
        "outputId": "4f70ff6e-c0d1-4df5-f986-80756b942d97"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ML-from-scratch' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "FJZVMwN1jaAc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8878dfe7-7f28-43ba-c02b-919746bd0c93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import bs4 as bs\n",
        "import re\n",
        "import urllib.request\n",
        "import warnings\n",
        "import pp\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Этот ноутбук будет дополнением к word2vec который был в курсе [NLP for you](https://lena-voita.github.io/nlp_course/word_embeddings.html) в главе Word Embeddings. Дополню я часть в которой берётся градиент от [Negative Log-Likelihood](https://github.com/testpassword/Machine-learning-and-data-analysis/blob/master/5%20-%20Логистическая%20регрессия/5.Логистическая_Регрессия.pdf), а точнее не берётся, это [Лена](https://github.com/lena-voita) оставила в качестве домашнего задания. Градиент по сути не сложный, но запутаться легко, да и к тому же при его взятии возникает один вопрос о котором мы и поговорим. Также считаю нужным объяснить смысл самой лосс функции: \"почему скалярное произведение?\"."
      ],
      "metadata": {
        "id": "wY3teFe6USmN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Начнём со смысла лосс функции. Вспомним для начала, что мы вообще хотим от наших эмбеддингов. Мы хотим чтобы слова похожие по смыслу, были близки друг к другу, то есть имели похожие векторные представления. Вспомним, что при фиксированной длине при увеличении скалярного произведение, расстояние между векторами уменьшается: это всё равно что уменьшать угол между векторами, так как при стремлении угла к 0, косинус угла увеличивается -> увеличивается скалярное произведение. Осталось подумать как не дать нашим векторам увеличивать скалярное произведение не за счёт увеличения своей длинны (в таком случае они будут отдаляться друг от друга), а за счёт уменьшения угла. Тут можно использовать [L2 регуляризацию](https://ml-handbook.ru/chapters/linear_models/intro#регуляризация)."
      ],
      "metadata": {
        "id": "bC5QN3mG5BDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$L(U, V) = -u_{context} \\cdot v_{central} + \\log{\\sum{_{w\\in V}} \\exp{(u_w \\cdot v_{central})}} $$"
      ],
      "metadata": {
        "id": "YWJcpImU2fD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\frac{\\partial L}{\\partial v_{central}} = -u_{central} + \\frac{1}{\\sum_{w \\in V}{\\exp{u_w \\cdot v_{central}}}} \\cdot \\sum_{w \\in V}[\\exp{(u_w \\cdot v_{central})}\\cdot u_{w}] $$"
      ],
      "metadata": {
        "id": "rUqm1lS02vEh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Если $ w = context \\text{,}$ тогда градиент равен"
      ],
      "metadata": {
        "id": "lOTak4sd236K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ \\frac{\\partial L}{\\partial u_{w}}=  -u_{central} +\\frac{1}{\\sum_{w \\in V}{\\exp({u_w v_{central}})}} \\cdot \\exp{(u_{context}  v_{central})}\\cdot v_{central}  $$\n"
      ],
      "metadata": {
        "id": "q0_sRvIn2x7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "при $w \\neq context \\text{:}$"
      ],
      "metadata": {
        "id": "Zr9-HI5X3Hfi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ \\frac{\\partial L}{\\partial u_{w}}=  \\frac{1}{\\sum_{w \\in V}{\\exp({u_w v_{central}})}} \\cdot \\exp{(u_w  v_{central})}\\cdot v_{central}  $$\n"
      ],
      "metadata": {
        "id": "FPJmZMYj3kFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word2vec**"
      ],
      "metadata": {
        "id": "bH3I-XZqm9uB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sites = [#'https://breakingbad.fandom.com/wiki/Walter_White',\n",
        "         'https://en.wikipedia.org/wiki/Breaking_Bad',\n",
        "        #  'https://breakingbad.fandom.com/wiki/Jesse_Pinkman',\n",
        "        #  'https://breakingbad.fandom.com/wiki/Gustavo_Fring', \n",
        "        # 'https://breakingbad.fandom.com/wiki/Jimmy_McGill',\n",
        "        # 'https://breakingbad.fandom.com/wiki/Mike_Ehrmantraut',\n",
        "        # 'https://breakingbad.fandom.com/wiki/Skyler_White',\n",
        "        # 'https://breakingbad.fandom.com/wiki/Hank_Schrader'\n",
        "         ]\n",
        "\n",
        "parser = pp.parse_and_prepare()\n",
        "vocab, text = parser.parse(sites)"
      ],
      "metadata": {
        "id": "5w4kVwSBgg1J"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class word2vec():\n",
        "    def __init__(self, iterations=5, learning_rate=0.1, l2=0.01, window=3, negative=5, vector_size=300):\n",
        "        self.iterations = iterations\n",
        "        self.learning_rate = learning_rate\n",
        "        self.l2 = l2\n",
        "        self.negative = negative\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "\n",
        "        self.V, self.U = self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        V = np.random.uniform(-0.6, 0.6, (len(vocab), self.vector_size))\n",
        "        U = np.random.uniform(-0.6, 0.6, (self.vector_size, len(vocab)))\n",
        "        return V, U\n",
        "\n",
        "    def computeGradient(self, central_vec, negative_sample): #\n",
        "        flag = 0\n",
        "        sum_exp_dot_cont = 0\n",
        "        for sample in negative_sample:\n",
        "            exponents = np.array([])\n",
        "            sample = np.array(sample)\n",
        "            for i in range(self.negative):\n",
        "                temp = np.exp(np.dot(sample[:, i], central_vec))\n",
        "                exponents = np.append(exponents, temp)\n",
        "                if flag == 0:\n",
        "                    sum_exp_dot_cont += temp * sample[:, i]\n",
        "\n",
        "            sum_exp = np.sum(exponents)\n",
        "            if flag == 0:\n",
        "                grad_central = (1 / sum_exp) * sum_exp_dot_cont - sample[:, self.negative - 1]\n",
        "                flag = 1\n",
        "\n",
        "            grad_negative_sample = []\n",
        "            grad_negative = []\n",
        "\n",
        "            for i in range(self.negative):\n",
        "                grad_negative.append((1 / sum_exp) * np.exp(np.dot(sample[:, i], central_vec)) * central_vec)\n",
        "\n",
        "            grad_negative[self.negative - 1] += -central_vec\n",
        "            grad_negative_sample.append(grad_negative)\n",
        "\n",
        "        return grad_central, grad_negative_sample\n",
        "\n",
        "    def computeRegularization(self, z):\n",
        "        # l2 reg for prohibit vectors from increasing the scalar product by \n",
        "        # increasing the size of the vector (that is, they can be separated)\n",
        "        return 2 * self.l2 * z\n",
        "        \n",
        "    def get_pos_in_vocab(self, vocab, word):\n",
        "        try:\n",
        "            for pos in range(len(vocab)):\n",
        "                if vocab[pos] == word:\n",
        "                    return pos\n",
        "        except ValueError:\n",
        "            print(\"This word isn't in dictionary:\", word)\n",
        "    \n",
        "    def get_negative_sample(self, context):\n",
        "        negative_sample = []\n",
        "        for i in range(len(context)):\n",
        "            random_index_sample = np.random.randint(self.vocab_size, size=self.negative)\n",
        "            random_index_sample[self.negative - 1] = context[i][1]\n",
        "            negative_sample_for_one = []\n",
        "            for k in range(self.negative):\n",
        "                negative_sample_for_one.append(self.U[:, random_index_sample[k]])  \n",
        "            negative_sample.append([negative_sample_for_one])\n",
        "            \n",
        "        return negative_sample\n",
        "\n",
        "    def get_context(self, text, pos_central):\n",
        "        context = []\n",
        "        cnt = 1\n",
        "        while cnt <= self.window and pos_central + cnt <= len(text):\n",
        "            context.append([text[pos_central+cnt], pos_central+cnt])\n",
        "            cnt = cnt + 1\n",
        "        cnt = 1\n",
        "        while cnt <= self.window and pos_central - cnt >= 0:\n",
        "            context.append([text[pos_central-cnt], pos_central-cnt])\n",
        "            cnt = cnt + 1\n",
        "        \n",
        "        return context\n",
        "\n",
        "    def update_weights(self, grad_central, grad_negative_sample, pos_central, context):\n",
        "        self.V[pos_central] -= self.learning_rate * grad_central + self.computeRegularization(self.V[pos_central])\n",
        "        for i in range(context):\n",
        "            self.U[:, context[i][1]] -= self.learning_rate * grad_negative_sample[i] + self.computeRegularization(self.U[:, pos_central])\n",
        "\n",
        "    def fit(self, vocab, text):\n",
        "        self.vocab_size = len(vocab)\n",
        "        for iteration in range(self.iterations):\n",
        "            for central_index, word in enumerate(text):\n",
        "                pos_central = self.get_pos_in_vocab(vocab, word)\n",
        "                pos_central = 4\n",
        "                context = self.get_context(text, pos_central) # \n",
        "                negative_sample = self.get_negative_sample(context)\n",
        "\n",
        "                grad_central, grad_negative_sample = self.computeGradient(self.V[pos_central], negative_sample)\n",
        "                self.update_weights(grad_central, grad_negative_sample, pos_central, context)\n",
        "        return self.U, self.V"
      ],
      "metadata": {
        "id": "11WLHgIGnlpm"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PPMI compare"
      ],
      "metadata": {
        "id": "om0tf1-c2slV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wv = word2vec()\n",
        "con = wv.fit(vocab, text)"
      ],
      "metadata": {
        "id": "PcgMAwc8DC6H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}